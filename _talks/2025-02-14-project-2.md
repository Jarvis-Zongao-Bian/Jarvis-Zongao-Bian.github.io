---
title: "🚀 Automating AI Model Training with CI/CD, Monitoring & Profiling – AI TrainOps"
collection: talks
# type: "Tutorial"
permalink: /talks/2025-02-13-project-2
# venue: "CatBoost & Optuna - PlaygroundS05E02 tutorial"
date: 2025-02-14
# location: "Kaggle"
---

[GitHub Repo Link](https://github.com/Jarvis-Zongao-Bian/AI-TrainOps)

## **🚀 Automating AI Model Training with CI/CD, Monitoring & Profiling – AI TrainOps**
#### **A Complete AI Training Workflow with Docker, GitHub Actions, Prometheus, and Profiling Tools**

---

### **📌 Introduction**
Training AI models efficiently is a challenge. Managing the **workflow**, **monitoring resource usage**, and **profiling performance bottlenecks** can be time-consuming.  

That's why I built **AI-TrainOps** – an automated AI training and optimization pipeline using:
✅ **GitHub Actions for CI/CD**  
✅ **Dockerized AI training workflow**  
✅ **Prometheus & Grafana for real-time monitoring**  
✅ **Performance profiling tools (perf, Valgrind, NVIDIA Nsight)**  

This project simplifies AI model training by **automating the entire pipeline**, improving efficiency, and enabling seamless deployment.

---

## **🚀 What I Built**
### **1️⃣ AI Model Training Pipeline**
I implemented an **end-to-end AI model training pipeline** using **PyTorch** to train a **ResNet** model on the **CIFAR-10 dataset**.

🔹 **Tech Stack:**  
- **PyTorch** for deep learning  
- **Docker** for containerized training  
- **GitHub Actions** for CI/CD automation  

📌 **How to Run Locally**
```bash
git clone https://github.com/Jarvis-Zongao-Bian/AI-TrainOps.git
cd AI-TrainOps
pip install -r requirements.txt
python src/train.py
```

---

### **2️⃣ Automating Training with Docker**
To ensure consistency across environments, I containerized the training workflow using **Docker**.

📌 **Build and Run Training in Docker**
```bash
docker build -t ai-trainops .
docker run --rm -v $(pwd)/logs:/app/logs ai-trainops
```

✅ **Why Docker?**
- Portable and runs consistently across machines  
- No dependency conflicts  
- Easy deployment and scaling  

---

### **3️⃣ CI/CD with GitHub Actions**
Every time I push new code, **GitHub Actions** automatically:
✅ **Builds the Docker image**  
✅ **Runs training & testing**  
✅ **Uploads training logs as artifacts**  

📌 **GitHub Actions Workflow**
```yaml
on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  train-test:
    runs-on: ubuntu-latest
    steps:
      - name: 🚀 Checkout Repository
        uses: actions/checkout@v4

      - name: 🛠️ Build Docker Image
        run: docker build -t ai-trainops .

      - name: 🏋️ Run Training
        run: docker run --rm -v $(pwd)/logs:/app/logs ai-trainops

      - name: 📂 Upload Logs
        uses: actions/upload-artifact@v4
        with:
          name: training-logs
          path: logs/*.log
```

📌 **Trigger the CI/CD Pipeline**
```bash
git add .
git commit -m "Trigger CI/CD pipeline"
git push origin main
```
👉 **[View the full GitHub Actions setup](https://github.com/Jarvis-Zongao-Bian/AI-TrainOps)**

---

### **4️⃣ Real-time Monitoring with Prometheus & Grafana**
To monitor **CPU usage, memory, and training loss**, I integrated **Prometheus & Grafana**.

📌 **Start Monitoring Services**
```bash
docker-compose up -d
```

✅ **Prometheus:** [`http://localhost:9090`](http://localhost:9090)  
✅ **Grafana:** [`http://localhost:4000`](http://localhost:4000) (Login: `admin / admin`)  

📌 **Query Training Metrics in Prometheus**
```
training_loss
cpu_usage_percent
memory_usage_mb
```

👉 **[Learn how to set up Grafana Dashboards](https://grafana.com/grafana/dashboards/)**

---

### **5️⃣ AI Training Performance Profiling**
To **optimize model training**, I added profiling tools:
✅ **perf** (CPU performance monitoring)  
✅ **Valgrind** (Memory leak detection)  
✅ **NVIDIA Nsight** (GPU performance analysis)  

📌 **Profile CPU Performance**
```bash
docker run --rm --privileged ai-trainops perf record -F 99 -g -- python3 src/train.py
docker run --rm --privileged ai-trainops perf report > logs/perf_report.txt
```

📌 **Detect Memory Leaks**
```bash
docker run --rm ai-trainops valgrind --tool=memcheck --leak-check=full python3 src/train.py > logs/valgrind_report.txt
```

👉 **[Learn more about NVIDIA Nsight profiling](https://developer.nvidia.com/nsight-systems)**

---

## **🎯 What’s Next?**
Now that AI-TrainOps is fully automated, my next steps are:
📊 **Enhance Grafana dashboards for deeper AI monitoring**  
📈 **Optimize model training with PyTorch Lightning**  
☁️ **Deploy as a cloud-based AI service**  

🚀 Want to contribute? **Fork the repo and submit a pull request!**  
👉 **[Check out AI-TrainOps on GitHub](https://github.com/Jarvis-Zongao-Bian/AI-TrainOps)**  

---

## **📌 Final Thoughts**
Building **AI-TrainOps** helped me **automate, optimize, and monitor AI training workflows**.  
If you're working on AI projects, I highly recommend **integrating CI/CD, monitoring, and profiling** into your pipeline.  

### **💡 Try It Yourself!**
1️⃣ **Clone the repo:**  
```bash
git clone https://github.com/Jarvis-Zongao-Bian/AI-TrainOps.git
cd AI-TrainOps
```
2️⃣ **Run training:**  
```bash
docker-compose up -d && docker run --rm ai-trainops
```
3️⃣ **Check monitoring at [`http://localhost:4000`](http://localhost:4000)**  

Let me know if you have any questions! 🚀✨

---

### **📌 Share Your Thoughts!**
🤖 **What’s your experience with AI model training automation?**  
📊 **How do you monitor your AI models in production?**  
Drop a comment below or **reach out on GitHub!** 🚀  

---
