---
title: "ğŸš€ Automating AI Model Training with CI/CD, Monitoring & Profiling â€“ AI TrainOps"
collection: talks
# type: "Tutorial"
permalink: /talks/2025-02-13-project-2
# venue: "CatBoost & Optuna - PlaygroundS05E02 tutorial"
date: 2025-02-14
# location: "Kaggle"
---

[GitHub Repo Link](https://github.com/Jarvis-Zongao-Bian/AI-TrainOps)

## **ğŸš€ Automating AI Model Training with CI/CD, Monitoring & Profiling â€“ AI TrainOps**
#### **A Complete AI Training Workflow with Docker, GitHub Actions, Prometheus, and Profiling Tools**

---

### **ğŸ“Œ Introduction**
Training AI models efficiently is a challenge. Managing the **workflow**, **monitoring resource usage**, and **profiling performance bottlenecks** can be time-consuming.  

That's why I built **AI-TrainOps** â€“ an automated AI training and optimization pipeline using:
âœ… **GitHub Actions for CI/CD**  
âœ… **Dockerized AI training workflow**  
âœ… **Prometheus & Grafana for real-time monitoring**  
âœ… **Performance profiling tools (perf, Valgrind, NVIDIA Nsight)**  

This project simplifies AI model training by **automating the entire pipeline**, improving efficiency, and enabling seamless deployment.

---

## **ğŸš€ What I Built**
### **1ï¸âƒ£ AI Model Training Pipeline**
I implemented an **end-to-end AI model training pipeline** using **PyTorch** to train a **ResNet** model on the **CIFAR-10 dataset**.

ğŸ”¹ **Tech Stack:**  
- **PyTorch** for deep learning  
- **Docker** for containerized training  
- **GitHub Actions** for CI/CD automation  

ğŸ“Œ **How to Run Locally**
```bash
git clone https://github.com/Jarvis-Zongao-Bian/AI-TrainOps.git
cd AI-TrainOps
pip install -r requirements.txt
python src/train.py
```

---

### **2ï¸âƒ£ Automating Training with Docker**
To ensure consistency across environments, I containerized the training workflow using **Docker**.

ğŸ“Œ **Build and Run Training in Docker**
```bash
docker build -t ai-trainops .
docker run --rm -v $(pwd)/logs:/app/logs ai-trainops
```

âœ… **Why Docker?**
- Portable and runs consistently across machines  
- No dependency conflicts  
- Easy deployment and scaling  

---

### **3ï¸âƒ£ CI/CD with GitHub Actions**
Every time I push new code, **GitHub Actions** automatically:
âœ… **Builds the Docker image**  
âœ… **Runs training & testing**  
âœ… **Uploads training logs as artifacts**  

ğŸ“Œ **GitHub Actions Workflow**
```yaml
on:
  push:
    branches:
      - main
  pull_request:
    branches:
      - main

jobs:
  train-test:
    runs-on: ubuntu-latest
    steps:
      - name: ğŸš€ Checkout Repository
        uses: actions/checkout@v4

      - name: ğŸ› ï¸ Build Docker Image
        run: docker build -t ai-trainops .

      - name: ğŸ‹ï¸ Run Training
        run: docker run --rm -v $(pwd)/logs:/app/logs ai-trainops

      - name: ğŸ“‚ Upload Logs
        uses: actions/upload-artifact@v4
        with:
          name: training-logs
          path: logs/*.log
```

ğŸ“Œ **Trigger the CI/CD Pipeline**
```bash
git add .
git commit -m "Trigger CI/CD pipeline"
git push origin main
```
ğŸ‘‰ **[View the full GitHub Actions setup](https://github.com/Jarvis-Zongao-Bian/AI-TrainOps)**

---

### **4ï¸âƒ£ Real-time Monitoring with Prometheus & Grafana**
To monitor **CPU usage, memory, and training loss**, I integrated **Prometheus & Grafana**.

ğŸ“Œ **Start Monitoring Services**
```bash
docker-compose up -d
```

âœ… **Prometheus:** [`http://localhost:9090`](http://localhost:9090)  
âœ… **Grafana:** [`http://localhost:4000`](http://localhost:4000) (Login: `admin / admin`)  

ğŸ“Œ **Query Training Metrics in Prometheus**
```
training_loss
cpu_usage_percent
memory_usage_mb
```

ğŸ‘‰ **[Learn how to set up Grafana Dashboards](https://grafana.com/grafana/dashboards/)**

---

### **5ï¸âƒ£ AI Training Performance Profiling**
To **optimize model training**, I added profiling tools:
âœ… **perf** (CPU performance monitoring)  
âœ… **Valgrind** (Memory leak detection)  
âœ… **NVIDIA Nsight** (GPU performance analysis)  

ğŸ“Œ **Profile CPU Performance**
```bash
docker run --rm --privileged ai-trainops perf record -F 99 -g -- python3 src/train.py
docker run --rm --privileged ai-trainops perf report > logs/perf_report.txt
```

ğŸ“Œ **Detect Memory Leaks**
```bash
docker run --rm ai-trainops valgrind --tool=memcheck --leak-check=full python3 src/train.py > logs/valgrind_report.txt
```

ğŸ‘‰ **[Learn more about NVIDIA Nsight profiling](https://developer.nvidia.com/nsight-systems)**

---

## **ğŸ¯ Whatâ€™s Next?**
Now that AI-TrainOps is fully automated, my next steps are:
ğŸ“Š **Enhance Grafana dashboards for deeper AI monitoring**  
ğŸ“ˆ **Optimize model training with PyTorch Lightning**  
â˜ï¸ **Deploy as a cloud-based AI service**  

ğŸš€ Want to contribute? **Fork the repo and submit a pull request!**  
ğŸ‘‰ **[Check out AI-TrainOps on GitHub](https://github.com/Jarvis-Zongao-Bian/AI-TrainOps)**  

---

## **ğŸ“Œ Final Thoughts**
Building **AI-TrainOps** helped me **automate, optimize, and monitor AI training workflows**.  
If you're working on AI projects, I highly recommend **integrating CI/CD, monitoring, and profiling** into your pipeline.  

### **ğŸ’¡ Try It Yourself!**
1ï¸âƒ£ **Clone the repo:**  
```bash
git clone https://github.com/Jarvis-Zongao-Bian/AI-TrainOps.git
cd AI-TrainOps
```
2ï¸âƒ£ **Run training:**  
```bash
docker-compose up -d && docker run --rm ai-trainops
```
3ï¸âƒ£ **Check monitoring at [`http://localhost:4000`](http://localhost:4000)**  

Let me know if you have any questions! ğŸš€âœ¨

---

### **ğŸ“Œ Share Your Thoughts!**
ğŸ¤– **Whatâ€™s your experience with AI model training automation?**  
ğŸ“Š **How do you monitor your AI models in production?**  
Drop a comment below or **reach out on GitHub!** ğŸš€  

---
